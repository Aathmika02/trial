# -*- coding: utf-8 -*-
"""parsing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vjq8VWLUBU2-6AtgY3d6jphc-Mze7NpJ
"""

import os
import re
import json
import torch
import argparse
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity


def parse_resume_file(file_path):
    # Load the BERT model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
    model = AutoModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
    model.eval()

    # Read the resume file and extract text
    with open(file_path, "r") as f:
        resume_text = f.read()

    # Split the resume text into sections using regular expressions
    name = re.search(r"([A-Z][a-z]+\s[A-Z][a-z]+)", resume_text).group(0)
    personal_details = re.findall(r"PERSONAL DETAILS\n(.+?)\n", resume_text)[0]
    education = re.findall(r"EDUCATION\n(.+?)\nEXPERIENCE", resume_text, re.DOTALL)[0]
    experience = re.findall(r"EXPERIENCE\n(.+?)\nSKILLS", resume_text, re.DOTALL)[0]
    skills = re.findall(r"SKILLS\n(.+)", resume_text, re.DOTALL)[0].split("\n")

    # Extract embeddings for each section of the resume
    name_embedding = get_embedding(model, tokenizer, name)
    personal_details_embedding = get_embedding(model, tokenizer, personal_details)
    education_embeddings = [get_embedding(model, tokenizer, e) for e in education.split("\n")]
    experience_embeddings = [get_embedding(model, tokenizer, e) for e in experience.split("\n")]
    skills_embeddings = [get_embedding(model, tokenizer, s) for s in skills]

    # Return a dictionary with the section name and its corresponding embedding
    resume_embeddings = {
        "name": name_embedding,
        "personal_details": personal_details_embedding,
        "education": education_embeddings,
        "experience": experience_embeddings,
        "skills": skills_embeddings
    }

    return resume_embeddings


def get_embedding(model, tokenizer, text):
    # Tokenize the text
    inputs = tokenizer(text, return_tensors="pt")

    # Generate the token embeddings
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].numpy()

    # Normalize the embeddings
    normalized_embeddings = embeddings / np.linalg.norm(embeddings)

    return normalized_embeddings


if __name__ == '__main__':
    # Set up argument parser
    parser = argparse.ArgumentParser(description='Parse a resume using ClinicalBERT.')
    parser.add_argument('--resume_path', type=str, required=True, help='path to the resume file')

    # Parse arguments
    args = parser.parse_args()

    # Call the function to parse the resume
    resume_embedding = parse_resume_file(args.resume_path)

    # Print the embeddings for each section of the resume
    for section, embedding in resume_embedding.items():
        print(section + " embedding:", embedding.tolist())